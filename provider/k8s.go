package provider

import (
	"bytes"
	"context"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"io"
	"io/ioutil"
	"math"
	"os"
	"os/exec"
	"sort"
	"strconv"
	"strings"
	"time"

	dp "github.com/novln/docker-parser"
	"github.com/segmentio/parquet-go"
	"go.uber.org/zap"
	"gocloud.dev/blob"
	_ "gocloud.dev/blob/fileblob"
	_ "gocloud.dev/blob/memblob"
	"golang.org/x/exp/slices"

	cfg "github.com/featureform/config"
	"github.com/featureform/helpers"
	"github.com/featureform/kubernetes"
	"github.com/featureform/logging"
	"github.com/featureform/metadata"
	pc "github.com/featureform/provider/provider_config"
)

const azureBlobStorePrefix = "abfss://"

// Hardcoded Go DateTime format, without milliseconds
// or UTC components, used for attempting to parse
// the timestamp column of an entity
const baseDateFormat = "2006-01-02 15:04:05"

type pandasOfflineQueries struct {
	defaultPythonOfflineQueries
}

func (q pandasOfflineQueries) trainingSetCreate(def TrainingSetDef, featureSchemas []ResourceSchema, labelSchema ResourceSchema) string {
	columns := make([]string, 0)
	joinQueries := make([]string, 0)
	featureTimestamps := make([]string, 0)
	for i, feature := range def.Features {
		featureColumnName := featureColumnName(feature)
		columns = append(columns, featureColumnName)
		var featureWindowQuery string
		// if no timestamp column, set to default generated by resource registration
		if featureSchemas[i].TS == "" {
			featureWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, 0 as t%d_ts FROM source_%d) ORDER BY t%d_ts ASC", featureSchemas[i].Entity, i+1, featureSchemas[i].Value, featureColumnName, i+1, i+1, i+1)
		} else {
			featureWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, %s as t%d_ts FROM source_%d) ORDER BY t%d_ts ASC", featureSchemas[i].Entity, i+1, featureSchemas[i].Value, featureColumnName, featureSchemas[i].TS, i+1, i+1, i+1)
		}
		featureJoinQuery := fmt.Sprintf("LEFT OUTER JOIN (%s) t%d ON (t%d_entity = entity AND t%d_ts <= label_ts)", featureWindowQuery, i+1, i+1, i+1)
		joinQueries = append(joinQueries, featureJoinQuery)
		featureTimestamps = append(featureTimestamps, fmt.Sprintf("t%d_ts", i+1))
	}
	for i, lagFeature := range def.LagFeatures {
		lagFeaturesOffset := len(def.Features)
		idx := slices.IndexFunc(def.Features, func(id ResourceID) bool {
			return id.Name == lagFeature.FeatureName && id.Variant == lagFeature.FeatureVariant
		})
		lagSource := fmt.Sprintf("source_%d", idx)
		lagColumnName := sanitize(lagFeature.LagName)
		if lagFeature.LagName == "" {
			lagColumnName = sanitize(fmt.Sprintf("%s_%s_lag_%s", lagFeature.FeatureName, lagFeature.FeatureVariant, lagFeature.LagDelta))
		}
		columns = append(columns, lagColumnName)
		timeDeltaSeconds := lagFeature.LagDelta.Seconds() //parquet stores time as microseconds
		curIdx := lagFeaturesOffset + i + 1
		var lagWindowQuery string
		if featureSchemas[idx].TS == "" {
			lagWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, 0 as t%d_ts FROM %s) ORDER BY t%d_ts ASC", featureSchemas[idx].Entity, curIdx, featureSchemas[idx].Value, lagColumnName, curIdx, lagSource, curIdx)
		} else {
			lagWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, %s as t%d_ts FROM %s) ORDER BY t%d_ts ASC", featureSchemas[idx].Entity, curIdx, featureSchemas[idx].Value, lagColumnName, featureSchemas[idx].TS, curIdx, lagSource, curIdx)
		}
		lagJoinQuery := fmt.Sprintf("LEFT OUTER JOIN (%s) t%d ON (t%d_entity = entity AND DATETIME(t%d_ts, '+%f seconds') <= label_ts)", lagWindowQuery, curIdx, curIdx, curIdx, timeDeltaSeconds)
		joinQueries = append(joinQueries, lagJoinQuery)
		featureTimestamps = append(featureTimestamps, fmt.Sprintf("t%d_ts", curIdx))
	}
	columnStr := strings.Join(columns, ", ")
	joinQueryString := strings.Join(joinQueries, " ")
	var labelWindowQuery string
	if labelSchema.TS == "" {
		labelWindowQuery = fmt.Sprintf("SELECT %s AS entity, %s AS value, 0 AS label_ts FROM source_0", labelSchema.Entity, labelSchema.Value)
	} else {
		labelWindowQuery = fmt.Sprintf("SELECT %s AS entity, %s AS value, %s AS label_ts FROM source_0", labelSchema.Entity, labelSchema.Value, labelSchema.TS)
	}
	labelPartitionQuery := fmt.Sprintf("(SELECT * FROM (SELECT entity, value, label_ts FROM (%s) t ) t0)", labelWindowQuery)
	labelJoinQuery := fmt.Sprintf("%s %s", labelPartitionQuery, joinQueryString)

	timeStamps := strings.Join(featureTimestamps, ", ")
	timeStampsDesc := strings.Join(featureTimestamps, " DESC,")
	fullQuery := fmt.Sprintf("SELECT %s, value AS %s, entity, label_ts, %s, ROW_NUMBER() over (PARTITION BY entity, value, label_ts ORDER BY label_ts DESC, %s DESC) as row_number FROM (%s) tt", columnStr, featureColumnName(def.Label), timeStamps, timeStampsDesc, labelJoinQuery)
	finalQuery := fmt.Sprintf("SELECT %s, %s FROM (SELECT * FROM (SELECT *, row_number FROM (%s) WHERE row_number=1 ))  ORDER BY label_ts", columnStr, featureColumnName(def.Label), fullQuery)
	return finalQuery
}

type K8sOfflineStore struct {
	executor Executor
	store    FileStore
	logger   *zap.SugaredLogger
	query    *pandasOfflineQueries
	BaseProvider
}

func (k8s *K8sOfflineStore) AsOfflineStore() (OfflineStore, error) {
	return k8s, nil
}

func (k8s *K8sOfflineStore) Close() error {
	return k8s.store.Close()
}

type Config []byte

type ExecutorFactory func(config Config, logger *zap.SugaredLogger) (Executor, error)

var executorFactoryMap = make(map[string]ExecutorFactory)

func RegisterExecutorFactory(name string, executorFactory ExecutorFactory) error {
	if _, exists := executorFactoryMap[name]; exists {
		return fmt.Errorf("factory already registered: %s", name)
	}
	executorFactoryMap[name] = executorFactory
	return nil
}

func CreateExecutor(name string, config Config, logger *zap.SugaredLogger) (Executor, error) {
	factory, exists := executorFactoryMap[name]
	if !exists {
		return nil, fmt.Errorf("factory does not exist: %s", name)
	}
	executor, err := factory(config, logger)
	if err != nil {
		return nil, err
	}
	return executor, nil
}

type FileStoreFactory func(config Config) (FileStore, error)

var fileStoreFactoryMap = make(map[string]FileStoreFactory)

func RegisterFileStoreFactory(name string, FileStoreFactory FileStoreFactory) error {
	if _, exists := fileStoreFactoryMap[name]; exists {
		return fmt.Errorf("factory already registered: %s", name)
	}
	fileStoreFactoryMap[name] = FileStoreFactory
	return nil
}

func CreateFileStore(name string, config Config) (FileStore, error) {
	factory, exists := fileStoreFactoryMap[name]
	if !exists {
		return nil, fmt.Errorf("factory does not exist: %s", name)
	}
	FileStore, err := factory(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create FileStore: %v", err)
	}
	return FileStore, nil
}

func init() {
	FileStoreFactoryMap := map[pc.FileStoreType]FileStoreFactory{
		pc.FileSystem: NewLocalFileStore,
		pc.Azure:      NewAzureFileStore,
		pc.S3:         NewS3FileStore,
		pc.GCS:        NewGCSFileStore,
		pc.HDFS:       NewHDFSFileStore,
	}
	executorFactoryMap := map[pc.ExecutorType]ExecutorFactory{
		pc.GoProc: NewLocalExecutor,
		pc.K8s:    NewKubernetesExecutor,
	}
	for storeType, factory := range FileStoreFactoryMap {
		err := RegisterFileStoreFactory(string(storeType), factory)
		if err != nil {
			panic(err)
		}
	}
	for executorType, factory := range executorFactoryMap {
		err := RegisterExecutorFactory(string(executorType), factory)
		if err != nil {
			panic(err)
		}
	}
}

func k8sOfflineStoreFactory(config pc.SerializedConfig) (Provider, error) {
	k8 := pc.K8sConfig{}
	logger := logging.NewLogger("kubernetes")
	if err := k8.Deserialize(config); err != nil {
		logger.Errorw("Invalid config to initialize k8s offline store", "error", err)
		return nil, fmt.Errorf("invalid k8s config: %w", err)
	}
	logger.Info("Creating executor with type:", k8.ExecutorType)
	execConfig := k8.ExecutorConfig.(pc.ExecutorConfig)
	serializedExecutor, err := execConfig.Serialize()
	if err != nil {
		logger.Errorw("Failure serializing executor", "executor_type", k8.ExecutorType, "error", err)
		return nil, err
	}
	executor, err := CreateExecutor(string(k8.ExecutorType), serializedExecutor, logger)
	if err != nil {
		logger.Errorw("Failure initializing executor", "executor_type", k8.ExecutorType, "error", err)
		return nil, err
	}

	serializedBlob, err := k8.StoreConfig.Serialize()
	if err != nil {
		return nil, fmt.Errorf("could not serialize blob store config")
	}

	logger.Info("Creating blob store with type:", k8.StoreType)
	store, err := CreateFileStore(string(k8.StoreType), serializedBlob)
	if err != nil {
		logger.Errorw("Failure initializing blob store with type", k8.StoreType, err)
		return nil, err
	}
	logger.Debugf("Store type: %s", k8.StoreType)
	queries := pandasOfflineQueries{}
	k8sOfflineStore := K8sOfflineStore{
		executor: executor,
		store:    store,
		logger:   logger,
		query:    &queries,
		BaseProvider: BaseProvider{
			ProviderType:   "K8S_OFFLINE",
			ProviderConfig: config,
		},
	}
	return &k8sOfflineStore, nil
}

type Executor interface {
	ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error
}

type LocalExecutor struct {
	scriptPath string
}

func (local LocalExecutor) ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error {
	envVars["MODE"] = "local"
	for key, value := range envVars {
		if err := os.Setenv(key, value); err != nil {
			return fmt.Errorf("could not set env variable: %s: %w", key, err)
		}
	}
	cmd := exec.Command("python3", local.scriptPath)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Run(); err != nil {
		return fmt.Errorf("could not execute python function: %v", err)
	}
	return nil
}

type LocalExecutorConfig struct {
	ScriptPath string
}

func (config *LocalExecutorConfig) Serialize() ([]byte, error) {
	data, err := json.Marshal(config)
	if err != nil {
		return nil, err
	}
	return data, nil
}

func (config *LocalExecutorConfig) Deserialize(data []byte) error {
	err := json.Unmarshal(data, config)
	if err != nil {
		return fmt.Errorf("deserialize executor config: %w", err)
	}
	return nil
}

func NewLocalExecutor(config Config, logger *zap.SugaredLogger) (Executor, error) {
	localConfig := LocalExecutorConfig{}
	if err := localConfig.Deserialize(config); err != nil {
		return nil, fmt.Errorf("failed to deserialize config")
	}
	_, err := os.Open(localConfig.ScriptPath)
	if err != nil {
		return nil, fmt.Errorf("could not find script path: %v", err)
	}
	return LocalExecutor{
		scriptPath: localConfig.ScriptPath,
	}, nil
}

type KubernetesExecutor struct {
	logger *zap.SugaredLogger
	image  string
}

// isDefaultImage checks that the current image name (excluding the tag) is the same as the default image
// name config.PandasBaseImage. It also validates that the name is a valid docker image name
func (kube *KubernetesExecutor) isDefaultImage() (bool, error) {
	parse, err := dp.Parse(kube.image)
	if err != nil {
		return false, fmt.Errorf("invalid image name: %w", err)
	}
	return parse.ShortName() == cfg.PandasBaseImage, nil
}

func (kube *KubernetesExecutor) setCustomImage(image string) {
	if image != "" {
		kube.image = image
	}
}

func (kube *KubernetesExecutor) ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error {
	kube.logger.Debugw("Executing k8s script", "args", args)
	var specs metadata.KubernetesResourceSpecs
	if args != nil {
		kube.setCustomImage(args.DockerImage)
		specs = args.Specs
	}
	if isDefault, err := kube.isDefaultImage(); err != nil {
		return fmt.Errorf("image check failed: %w", err)
	} else if !isDefault {
		kube.logger.Warnf("You are using a custom Docker Image (%s) for a Kubernetes job. This may have unintended behavior.", kube.image)
	}
	envVars["MODE"] = "k8s"
	resourceType, err := strconv.Atoi(envVars["RESOURCE_TYPE"])
	if err != nil {
		resourceType = 0
	}
	config := kubernetes.KubernetesRunnerConfig{
		JobPrefix: "kcf",
		EnvVars:   envVars,
		Image:     kube.image,
		NumTasks:  1,
		Resource: metadata.ResourceID{
			Name:    envVars["RESOURCE_NAME"],
			Variant: envVars["RESOURCE_VARIANT"],
			Type:    ProviderToMetadataResourceType[OfflineResourceType(resourceType)],
		},
		Specs: specs,
	}
	jobRunner, err := kubernetes.NewKubernetesRunner(config)
	if err != nil {
		return err
	}
	completionWatcher, err := jobRunner.Run()
	if err != nil {
		return err
	}
	if err := completionWatcher.Wait(); err != nil {
		return err
	}
	return nil
}

func NewKubernetesExecutor(config Config, logger *zap.SugaredLogger) (Executor, error) {
	var c pc.ExecutorConfig
	err := c.Deserialize(config)
	if err != nil {
		return nil, fmt.Errorf("could not create Kubernetes Executor: %w", err)
	}
	return &KubernetesExecutor{
		image:  c.GetImage(),
		logger: logger,
	}, nil
}

type FileStore interface {
	Write(key string, data []byte) error
	Writer(key string) (*blob.Writer, error)
	Read(key string) ([]byte, error)
	Serve(key string) (Iterator, error)
	Exists(key string) (bool, error)
	Delete(key string) error
	DeleteAll(dir string) error
	NewestFileOfType(prefix string, fileType FileType) (string, error)
	PathWithPrefix(path string, remote bool) string
	NumRows(key string) (int64, error)
	Close() error
	Upload(sourcePath string, destPath string) error
	Download(sourcePath string, destPath string) error
	FilestoreType() pc.FileStoreType
	AddEnvVars(envVars map[string]string) map[string]string
}

type Iterator interface {
	Next() (map[string]interface{}, error)
	FeatureColumns() []string
	LabelColumn() string
}

type genericFileStore struct {
	bucket *blob.Bucket
	path   string
}

func (store *genericFileStore) PathWithPrefix(path string, remote bool) string {
	// What does this mean? Change this as check for local file
	if len(store.path) > 4 && store.path[0:4] == "file" {
		return fmt.Sprintf("%s%s", store.path[len("file:///"):], strings.TrimPrefix(path, "/"))
	} else {
		return path
	}
}

func (store *genericFileStore) NewestFileOfType(prefix string, fileType FileType) (string, error) {
	opts := blob.ListOptions{
		Prefix: prefix,
	}
	listIterator := store.bucket.List(&opts)
	mostRecentTime := time.UnixMilli(0)
	mostRecentKey := ""
	for {
		if newObj, err := listIterator.Next(context.TODO()); err == nil {
			mostRecentTime, mostRecentKey = store.getMoreRecentFile(newObj, fileType, mostRecentTime, mostRecentKey)
		} else if err == io.EOF {
			return mostRecentKey, nil
		} else {
			return "", err
		}
	}
}

func (store *genericFileStore) getMoreRecentFile(newObj *blob.ListObject, expectedFileType FileType, oldTime time.Time, oldKey string) (time.Time, string) {
	pathParts := strings.Split(newObj.Key, ".")
	fileType := pathParts[len(pathParts)-1]
	if fileType == string(expectedFileType) && !newObj.IsDir && store.isMostRecentFile(newObj, oldTime) {
		return newObj.ModTime, newObj.Key
	}
	return oldTime, oldKey
}

func (store *genericFileStore) isMostRecentFile(listObj *blob.ListObject, time time.Time) bool {
	return listObj.ModTime.After(time) || listObj.ModTime.Equal(time)
}

func (store *genericFileStore) outputFileList(prefix string) []string {
	opts := blob.ListOptions{
		Prefix:    prefix,
		Delimiter: "/",
	}
	listIterator := store.bucket.List(&opts)
	mostRecentOutputPartTime := "0000-00-00 00:00:00.000000"
	mostRecentOutputPartPath := ""
	for listObj, err := listIterator.Next(context.TODO()); err == nil; listObj, err = listIterator.Next(context.TODO()) {
		if listObj == nil {
			return []string{}
		}
		dirParts := strings.Split(listObj.Key[:len(listObj.Key)-1], "/")
		timestamp := dirParts[len(dirParts)-1]
		if listObj.IsDir && timestamp > mostRecentOutputPartTime {
			mostRecentOutputPartTime = timestamp
			mostRecentOutputPartPath = listObj.Key
		}
	}
	opts = blob.ListOptions{
		Prefix: mostRecentOutputPartPath,
	}
	partsIterator := store.bucket.List(&opts)
	partsList := make([]string, 0)
	for listObj, err := partsIterator.Next(context.TODO()); err == nil; listObj, err = partsIterator.Next(context.TODO()) {
		pathParts := strings.Split(listObj.Key, ".")

		fileType := pathParts[len(pathParts)-1]
		if fileType == "parquet" {
			partsList = append(partsList, listObj.Key)
		}
	}
	sort.Strings(partsList)
	return partsList
}

func (store *genericFileStore) DeleteAll(dir string) error {
	opts := blob.ListOptions{
		Prefix: dir,
	}
	listIterator := store.bucket.List(&opts)
	for listObj, err := listIterator.Next(context.TODO()); err == nil; listObj, err = listIterator.Next(context.TODO()) {
		if !listObj.IsDir {
			if err := store.bucket.Delete(context.TODO(), listObj.Key); err != nil {
				return fmt.Errorf("failed to delete object %s in directory %s: %v", listObj.Key, dir, err)
			}
		}
	}
	return nil
}

func (store *genericFileStore) Write(key string, data []byte) error {
	err := store.bucket.WriteAll(context.TODO(), key, data, nil)
	if err != nil {
		return err
	}
	return nil
}

func (store *genericFileStore) Writer(key string) (*blob.Writer, error) {
	return store.bucket.NewWriter(context.TODO(), key, nil)
}

func (store *genericFileStore) Read(key string) ([]byte, error) {
	data, err := store.bucket.ReadAll(context.TODO(), key)
	if err != nil {
		return nil, err
	}
	return data, nil
}

func (store *genericFileStore) ServeDirectory(dir string) (Iterator, error) {
	fileParts := store.outputFileList(dir)
	if len(fileParts) == 0 {
		return nil, fmt.Errorf("no files in given directory")
	}
	// assume file type is parquet
	return parquetIteratorOverMultipleFiles(fileParts, store)
}

func (store *genericFileStore) Upload(sourcePath string, destPath string) error {
	content, err := ioutil.ReadFile(sourcePath)
	if err != nil {
		return fmt.Errorf("cannot read %s file: %v", sourcePath, err)
	}

	err = store.Write(destPath, content)
	if err != nil {
		return fmt.Errorf("cannot upload %s file to %s destination: %v", sourcePath, destPath, err)
	}

	return nil
}

func (store *genericFileStore) Download(sourcePath string, destPath string) error {
	content, err := store.Read(sourcePath)
	if err != nil {
		return fmt.Errorf("cannot read %s file: %v", sourcePath, err)
	}

	f, err := os.Create(destPath)
	if err != nil {
		return fmt.Errorf("cannot create %s file: %v", destPath, err)
	}
	defer f.Close()

	f.Write(content)

	return nil
}

func (store *genericFileStore) FilestoreType() pc.FileStoreType {
	return Memory
}

func (store *genericFileStore) AddEnvVars(envVars map[string]string) map[string]string {
	return envVars
}

func convertToParquetBytes(list []any) ([]byte, error) {
	// TODO possibly accepts single struct instead of list, have to be able to accept either, or another function
	if len(list) == 0 {
		return nil, fmt.Errorf("list is empty")
	}
	schema := parquet.SchemaOf(list[0])
	buf := new(bytes.Buffer)
	err := parquet.Write[any](
		buf,
		list,
		schema,
	)
	if err != nil {
		return nil, fmt.Errorf("could not write parquet file to bytes: %v", err)
	}
	return buf.Bytes(), nil
}

type ParquetIteratorMultipleFiles struct {
	fileList       []string
	currentFile    int64
	fileIterator   Iterator
	featureColumns []string
	labelColumn    string
	store          FileStore
}

func parquetIteratorOverMultipleFiles(fileParts []string, store FileStore) (Iterator, error) {
	b, err := store.Read(fileParts[0])
	//b, err := store.bucket.ReadAll(context.TODO(), fileParts[0])
	if err != nil {
		return nil, fmt.Errorf("could not read bucket: %w", err)
	}
	iterator, err := parquetIteratorFromBytes(b)
	if err != nil {
		return nil, fmt.Errorf("could not open first parquet file: %w", err)
	}
	return &ParquetIteratorMultipleFiles{
		fileList:     fileParts,
		currentFile:  int64(0),
		fileIterator: iterator,
		store:        store,
	}, nil
}

func (p *ParquetIteratorMultipleFiles) FeatureColumns() []string {
	return p.featureColumns
}

func (p *ParquetIteratorMultipleFiles) LabelColumn() string {
	return p.labelColumn
}

func (p *ParquetIteratorMultipleFiles) Next() (map[string]interface{}, error) {
	nextRow, err := p.fileIterator.Next()
	if err != nil {
		return nil, err
	}
	if nextRow == nil {
		if p.currentFile+1 == int64(len(p.fileList)) {
			return nil, nil
		}
		p.currentFile += 1
		b, err := p.store.Read(p.fileList[p.currentFile])
		if err != nil {
			return nil, err
		}
		iterator, err := parquetIteratorFromBytes(b)
		if err != nil {
			return nil, err
		}
		p.fileIterator = iterator
		return p.fileIterator.Next()
	}
	return nextRow, nil
}

func (store *genericFileStore) Serve(key string) (Iterator, error) {
	keyParts := strings.Split(key, ".")
	if len(keyParts) == 1 {
		return store.ServeDirectory(key)
	}
	b, err := store.bucket.ReadAll(context.TODO(), key)
	if err != nil {
		return nil, fmt.Errorf("could not read file: %w", err)
	}
	switch fileType := keyParts[len(keyParts)-1]; fileType {
	case "parquet":
		return parquetIteratorFromBytes(b)
	case "csv":
		return nil, fmt.Errorf("csv iterator not implemented")
	default:
		return nil, fmt.Errorf("unsupported file type")
	}

}

func (store *genericFileStore) NumRows(key string) (int64, error) {
	b, err := store.bucket.ReadAll(context.TODO(), key)
	if err != nil {
		return 0, err
	}
	keyParts := strings.Split(key, ".")
	switch fileType := keyParts[len(keyParts)-1]; fileType {
	case "parquet":
		return getParquetNumRows(b)
	default:
		return 0, fmt.Errorf("unsupported file type")
	}
}

type csvIterator struct {
	reader        *csv.Reader
	currentValues GenericRecord
	err           error
	columnNames   []string
	idx           int64
	limit         int64
}

func (c *csvIterator) Next() bool {
	if c.idx >= c.limit {
		return false
	}
	row, err := c.reader.Read()
	if err != nil {
		if err == io.EOF {
			return false
		} else {
			c.err = err
			return false
		}
	}
	c.currentValues = c.ParseRow(row)
	c.idx += 1
	return true
}

func (c *csvIterator) Values() GenericRecord {
	return c.currentValues
}

func (c *csvIterator) Columns() []string {
	return c.columnNames
}

func (c *csvIterator) Err() error {
	return c.err
}

func (c *csvIterator) Close() error {
	return nil
}

func (c *csvIterator) ParseRow(row []string) GenericRecord {
	records := make(GenericRecord, len(row))
	for i, value := range row {
		if integer, err := strconv.Atoi(value); err == nil {
			records[i] = integer
			continue
		}
		if float, err := strconv.ParseFloat(value, 64); err == nil {
			records[i] = float
			continue
		}
		records[i] = value
	}
	return records
}

func newCSVIterator(b []byte, limit int64) (GenericTableIterator, error) {
	reader := csv.NewReader(bytes.NewReader(b))
	headers, err := reader.Read()
	if err != nil {
		return nil, fmt.Errorf("failed to create CSV reader: %w", err)
	}
	if limit == -1 {
		limit = math.MaxInt64
	}
	return &csvIterator{
		reader:      reader,
		columnNames: headers,
		limit:       limit,
		idx:         0,
	}, nil
}

type parquetIterator struct {
	reader        *parquet.Reader
	currentValues GenericRecord
	err           error
	columnNames   []string
	limit         int64
	idx           int64
}

func (p *parquetIterator) Next() bool {
	if p.idx >= p.limit {
		return false
	}
	value := make(map[string]interface{})
	err := p.reader.Read(&value)
	if err != nil {
		if err == io.EOF {
			return false
		} else {
			p.err = err
			return false
		}
	}
	records := make(GenericRecord, 0)
	for idx := range p.columnNames {
		records = append(records, value[p.columnNames[idx]])
	}
	p.currentValues = records
	p.idx += 1
	return true
}

func (p *parquetIterator) Values() GenericRecord {
	return p.currentValues
}

func (p *parquetIterator) Columns() []string {
	return p.columnNames
}

func (p *parquetIterator) Err() error {
	return p.err
}

func (p *parquetIterator) Close() error {
	return p.reader.Close()
}

func newParquetIterator(b []byte, limit int64) (GenericTableIterator, error) {
	file := bytes.NewReader(b)
	r := parquet.NewReader(file)
	columnList := r.Schema().Columns()
	columnNames := make([]string, len(columnList))
	for i, column := range columnList {
		columnNames[i] = column[0]
	}
	if limit == -1 {
		limit = math.MaxInt64
	}
	return &parquetIterator{
		reader:      r,
		columnNames: columnNames,
		limit:       limit,
		idx:         0,
	}, nil
}

type ParquetIterator struct {
	reader         *parquet.Reader
	index          int64
	featureColumns []string
	labelColumn    string
}

func (p *ParquetIterator) Next() (map[string]interface{}, error) {
	value := make(map[string]interface{})
	err := p.reader.Read(&value)
	if err != nil {
		if err == io.EOF {
			return nil, nil
		} else {
			return nil, err
		}
	}
	return value, nil
}

func (p *ParquetIterator) FeatureColumns() []string {
	return p.featureColumns
}

func (p *ParquetIterator) LabelColumn() string {
	return p.labelColumn
}

func getParquetNumRows(b []byte) (int64, error) {
	file := bytes.NewReader(b)
	r := parquet.NewReader(file)
	return r.NumRows(), nil
}

type columnType string

const (
	labelType   columnType = "Label"
	featureType            = "Feature"
)

type parquetSchema struct {
	featureColumns []string
	labelColumn    string
}

func (s *parquetSchema) parseParquetColumnName(r *parquet.Reader) {
	columnList := r.Schema().Columns()
	for _, column := range columnList {
		columnName := column[0]
		colType := s.getColumnType(columnName)
		s.setColumn(colType, columnName)
	}
}
func (s *parquetSchema) getColumnType(name string) columnType {
	columnSections := strings.Split(name, "__")
	return columnType(columnSections[0])
}

func (s *parquetSchema) setColumn(colType columnType, name string) {
	if colType == labelType {
		s.labelColumn = name
	} else if colType == featureType {
		s.featureColumns = append(s.featureColumns, name)
	}
}

func parquetIteratorFromBytes(b []byte) (Iterator, error) {
	file := bytes.NewReader(b)
	r := parquet.NewReader(file)
	schema := parquetSchema{}
	schema.parseParquetColumnName(r)
	return &ParquetIterator{
		reader:         r,
		index:          int64(0),
		featureColumns: schema.featureColumns,
		labelColumn:    schema.labelColumn,
	}, nil
}

func (store *genericFileStore) Exists(key string) (bool, error) {
	return store.bucket.Exists(context.TODO(), key)
}

func (store *genericFileStore) Delete(key string) error {
	return store.bucket.Delete(context.TODO(), key)
}

func (store *genericFileStore) Close() error {
	return store.bucket.Close()
}

func ResourcePrefix(id ResourceID) string {
	return fmt.Sprintf("featureform/%s/%s/%s", id.Type, id.Name, id.Variant)
}

func fileStoreResourcePath(id ResourceID) string {
	return ResourcePrefix(id)
}

type BlobOfflineTable struct {
	schema ResourceSchema
}

func (tbl *BlobOfflineTable) Write(ResourceRecord) error {
	return fmt.Errorf("not yet implemented")
}

func (k8s *K8sOfflineStore) RegisterResourceFromSourceTable(id ResourceID, schema ResourceSchema) (OfflineTable, error) {
	return blobRegisterResource(id, schema, k8s.logger, k8s.store)
}

func blobRegisterResource(id ResourceID, schema ResourceSchema, logger *zap.SugaredLogger, store FileStore) (OfflineTable, error) {
	if err := id.check(Feature, Label); err != nil {
		logger.Errorw("Failure checking ID", "error", err)
		return nil, fmt.Errorf("ID check failed: %v", err)
	}
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	resourceExists, err := store.Exists(resourceKey)
	if err != nil {
		logger.Errorw("Error checking if resource exists", "error", err)
		return nil, fmt.Errorf("error checking if resource registry exists: %v", err)
	}
	if resourceExists {
		logger.Errorw("Resource already exists in blob store", "id", id, "ResourceKey", resourceKey)
		return nil, &TableAlreadyExists{id.Name, id.Variant}
	}
	serializedSchema, err := schema.Serialize()
	if err != nil {
		return nil, fmt.Errorf("error serializing resource schema: %s: %s", schema, err)
	}
	if err := store.Write(resourceKey, serializedSchema); err != nil {
		return nil, fmt.Errorf("error writing resource schema: %s: %s", schema, err)
	}
	logger.Debugw("Registered resource table", "resourceID", id, "for source", schema.SourceTable)
	return &BlobOfflineTable{schema}, nil
}

type FileStorePrimaryTable struct {
	store            FileStore
	sourcePath       string
	isTransformation bool
	id               ResourceID
}

func (tbl *FileStorePrimaryTable) Write(GenericRecord) error {
	return fmt.Errorf("not implemented")
}

func (tbl *FileStorePrimaryTable) GetName() string {
	return tbl.sourcePath
}

func (tbl *FileStorePrimaryTable) IterateSegment(n int64) (GenericTableIterator, error) {
	path := tbl.sourcePath
	keyParts := strings.Split(path, ".")
	if len(keyParts) == 1 {
		return nil, fmt.Errorf("expected a file but got a directory: %s", keyParts[0])
	}
	b, err := tbl.store.Read(path)
	if err != nil {
		return nil, fmt.Errorf("could not read file: %w", err)
	}
	switch fileType := keyParts[len(keyParts)-1]; fileType {
	case "parquet":
		return newParquetIterator(b, n)
	case "csv":
		return newCSVIterator(b, n)
	default:
		return nil, fmt.Errorf("unsupported file type: %s", fileType)
	}
}

func (tbl *FileStorePrimaryTable) NumRows() (int64, error) {
	return tbl.store.NumRows(tbl.sourcePath)
}

type FileStoreIterator struct {
	iter    Iterator
	err     error
	curIdx  int64
	maxIdx  int64
	records []interface{}
	columns []string
}

func (it *FileStoreIterator) Next() bool {
	it.curIdx += 1
	if it.curIdx > it.maxIdx {
		return false
	}
	values, err := it.iter.Next()
	if values == nil {
		return false
	}
	if err != nil {
		it.err = err
		return false
	}
	records := make([]interface{}, 0)
	columns := make([]string, 0)
	for k, v := range values {
		columns = append(columns, k)
		records = append(records, v)
	}
	it.columns = columns
	it.records = records
	return true
}

func (it *FileStoreIterator) Columns() []string {
	return it.columns
}

func (it *FileStoreIterator) Err() error {
	return it.err
}

func (it *FileStoreIterator) Values() GenericRecord {
	return it.records
}

func (it *FileStoreIterator) Close() error {
	return nil
}

func (k8s *K8sOfflineStore) RegisterPrimaryFromSourceTable(id ResourceID, sourceName string) (PrimaryTable, error) {
	return blobRegisterPrimary(id, sourceName, k8s.logger, k8s.store)
}

func blobRegisterPrimary(id ResourceID, sourceName string, logger *zap.SugaredLogger, store FileStore) (PrimaryTable, error) {
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	logger.Infow("Checking if resource key exists", "key", resourceKey)
	primaryExists, err := store.Exists(resourceKey)
	if err != nil {
		logger.Errorw("Error checking if primary exists", "error", err)
		return nil, fmt.Errorf("error checking if primary exists: %v", err)
	}
	if primaryExists {
		logger.Errorw("Primary table already exists", "source", sourceName)
		return nil, fmt.Errorf("primary already exists")
	}

	logger.Debugw("Registering primary table", "id", id, "source", sourceName)
	if err := store.Write(resourceKey, []byte(sourceName)); err != nil {
		logger.Errorw("Could not write primary table", "error", err)
		return nil, err
	}

	logger.Debugw("Successfully registered primary table", "id", id, "source", sourceName)
	return &FileStorePrimaryTable{store, sourceName, false, id}, nil
}

func (k8s *K8sOfflineStore) CreateTransformation(config TransformationConfig) error {
	return k8s.transformation(config, false)
}

func (k8s *K8sOfflineStore) transformation(config TransformationConfig, isUpdate bool) error {
	if config.Type == SQLTransformation {
		return k8s.sqlTransformation(config, isUpdate)
	} else if config.Type == DFTransformation {
		return k8s.dfTransformation(config, isUpdate)
	} else {
		k8s.logger.Errorw("the transformation type is not supported", "type", config.Type)
		return fmt.Errorf("the transformation type '%v' is not supported", config.Type)
	}
}

func addETCDVars(envVars map[string]string) map[string]string {
	etcdHost := helpers.GetEnv("ETCD_HOST", "localhost")
	etcdPort := helpers.GetEnv("ETCD_PORT", "2379")
	etcdPassword := helpers.GetEnv("ETCD_PASSWORD", "secretpassword")
	etcdUsername := helpers.GetEnv("ETCD_USERNAME", "root")
	envVars["ETCD_HOST"] = etcdHost
	envVars["ETCD_PASSWORD"] = etcdPassword
	envVars["ETCD_PORT"] = etcdPort
	envVars["ETCD_USERNAME"] = etcdUsername
	return envVars
}

func (k8s *K8sOfflineStore) pandasRunnerArgs(outputURI string, updatedQuery string, sources []string, jobType JobType) map[string]string {
	sourceList := strings.Join(sources, ",")
	envVars := map[string]string{
		"OUTPUT_URI":          outputURI,
		"SOURCES":             sourceList,
		"TRANSFORMATION_TYPE": "sql",
		"TRANSFORMATION":      updatedQuery,
	}
	envVars = k8s.store.AddEnvVars(envVars)
	return envVars
}

func (k8s K8sOfflineStore) getDFArgs(outputURI string, code string, mapping []SourceMapping, sources []string) map[string]string {
	sourceList := strings.Join(sources, ",")
	envVars := map[string]string{
		"OUTPUT_URI":          outputURI,
		"SOURCES":             sourceList,
		"TRANSFORMATION_TYPE": "df",
		"TRANSFORMATION":      code,
	}
	envVars = k8s.store.AddEnvVars(envVars)
	return envVars
}

func addResourceID(envVars map[string]string, id ResourceID) map[string]string {
	envVars["RESOURCE_NAME"] = id.Name
	envVars["RESOURCE_VARIANT"] = id.Variant
	envVars["RESOURCE_TYPE"] = fmt.Sprintf("%d", id.Type)
	return envVars
}

func (k8s *K8sOfflineStore) sqlTransformation(config TransformationConfig, isUpdate bool) error {
	updatedQuery, sources, err := k8s.updateQuery(config.Query, config.SourceMapping)
	if err != nil {
		k8s.logger.Errorw("Could not generate updated query for k8s transformation", err)
		return err
	}

	transformationDestination := k8s.store.PathWithPrefix(fileStoreResourcePath(config.TargetTableID), false)
	transformationDestinationExactPath, err := k8s.store.NewestFileOfType(transformationDestination, Parquet)
	if err != nil {
		k8s.logger.Errorw("Could not get newest blob", "location", transformationDestination, "error", err)
		return fmt.Errorf("could not get newest blob: %s: %v", transformationDestination, err)
	}
	exists := transformationDestinationExactPath != ""
	if !isUpdate && exists {
		k8s.logger.Errorw("Creation when transformation already exists", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v already exists at %s", config.TargetTableID, transformationDestination)
	} else if isUpdate && !exists {
		k8s.logger.Errorw("Update job attempted when transformation does not exist", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v doesn't exist at %s and you are trying to update", config.TargetTableID, transformationDestination)
	}
	k8s.logger.Debugw("Running SQL transformation", "target_table", config.TargetTableID, "query", config.Query)
	runnerArgs := k8s.pandasRunnerArgs(transformationDestination, updatedQuery, sources, Transform)

	runnerArgs = addResourceID(runnerArgs, config.TargetTableID)
	args, err := k8s.checkArgs(config.Args)
	if err != nil {
		return fmt.Errorf("could not check args: %w", err)
	}
	if err := k8s.executor.ExecuteScript(runnerArgs, &args); err != nil {
		k8s.logger.Errorw("job for transformation failed to run", "target_table", config.TargetTableID, "error", err)
		return fmt.Errorf("job for transformation %v failed to run: %v", config.TargetTableID, err)
	}

	k8s.logger.Debugw("Successfully ran SQL transformation", "target_table", config.TargetTableID, "query", config.Query)
	return nil
}

func (k8s *K8sOfflineStore) checkArgs(args metadata.TransformationArgs) (metadata.KubernetesArgs, error) {
	k8sArgs, ok := args.(metadata.KubernetesArgs)
	if !ok {
		return metadata.KubernetesArgs{}, fmt.Errorf("invalid type used for Kubernetes Arguments")
	}
	return k8sArgs, nil
}

func (k8s *K8sOfflineStore) dfTransformation(config TransformationConfig, isUpdate bool) error {
	_, sources, err := k8s.updateQuery(config.Query, config.SourceMapping)
	if err != nil {
		return err
	}
	transformationDestination := k8s.store.PathWithPrefix(fileStoreResourcePath(config.TargetTableID), false)
	exists, err := k8s.store.Exists(transformationDestination)
	if err != nil {
		k8s.logger.Errorw("Error checking if resource exists", "error", err)
		return err
	}

	if !isUpdate && exists {
		k8s.logger.Errorw("Transformation already exists", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v already exists at %s", config.TargetTableID, transformationDestination)
	} else if isUpdate && !exists {
		k8s.logger.Errorw("Transformation doesn't exists at destination and you are trying to update", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v doesn't exist at %s and you are trying to update", config.TargetTableID, transformationDestination)
	}

	transformationFilePath := k8s.store.PathWithPrefix(fileStoreResourcePath(config.TargetTableID), false)
	fileName := "transformation.pkl"
	transformationFileLocation := fmt.Sprintf("%s%s", transformationFilePath, fileName)
	err = k8s.store.Write(transformationFileLocation, config.Code)
	if err != nil {
		return fmt.Errorf("could not upload file: %v", err)
	}

	dfArgs := k8s.getDFArgs(transformationDestination, transformationFileLocation, config.SourceMapping, sources)
	dfArgs = addResourceID(dfArgs, config.TargetTableID)
	k8s.logger.Debugw("Running DF transformation", "target_table", config.TargetTableID)
	args, err := k8s.checkArgs(config.Args)
	if err != nil {
		return fmt.Errorf("could not check args: %w", err)
	}
	if err := k8s.executor.ExecuteScript(dfArgs, &args); err != nil {
		k8s.logger.Errorw("Error running dataframe job", "error", err)
		return fmt.Errorf("submit job for transformation %v failed to run: %v", config.TargetTableID, err)
	}

	k8s.logger.Debugw("Successfully ran DF transformation", "target_table", config.TargetTableID)
	return nil
}

func (k8s *K8sOfflineStore) updateQuery(query string, mapping []SourceMapping) (string, []string, error) {
	sources := make([]string, len(mapping))
	replacements := make([]string, len(mapping)*2) // It's times 2 because each replacement will be a pair; (original, replacedValue)

	for i, m := range mapping {
		replacements = append(replacements, m.Template)
		replacements = append(replacements, fmt.Sprintf("source_%v", i))

		sourcePath := ""
		sourcePath, err := k8s.getSourcePath(m.Source)
		k8s.logger.Debugw("Fetched Source Path", "source", m.Source, "sourcePath", sourcePath)
		if err != nil {
			k8s.logger.Errorw("Error getting source path of source", "source", m.Source, "error", err)
			return "", nil, fmt.Errorf("could not get the sourcePath for %s because %s", m.Source, err)
		}

		sources[i] = sourcePath
	}

	replacer := strings.NewReplacer(replacements...)
	updatedQuery := replacer.Replace(query)

	if strings.Contains(updatedQuery, "{{") {
		k8s.logger.Errorw("Template replace failed", "query", updatedQuery)
		return "", nil, fmt.Errorf("could not replace all the templates with the current mapping. Mapping: %v; Replaced Query: %s", mapping, updatedQuery)
	}
	return updatedQuery, sources, nil
}

func (k8s *K8sOfflineStore) getSourcePath(path string) (string, error) {
	fileType, fileName, fileVariant := k8s.getResourceInformationFromFilePath(path)
	k8s.logger.Debugw("Retrieved source path", "fileType", fileType, "fileName", fileName, "fileVariant", fileVariant)

	var filePath string
	if fileType == "primary" {
		fileResourceId := ResourceID{Name: fileName, Variant: fileVariant, Type: Primary}
		fileTable, err := k8s.GetPrimaryTable(fileResourceId)
		if err != nil {
			k8s.logger.Errorw("Issue getting primary table", "id", fileResourceId, "error", err)
			return "", fmt.Errorf("could not get the primary table for {%v} because %s", fileResourceId, err)
		}
		filePath = fileTable.GetName()
		return filePath, nil
	} else if fileType == "transformation" {
		fileResourceId := ResourceID{Name: fileName, Variant: fileVariant, Type: Transformation}
		fileResourcePath := k8s.store.PathWithPrefix(fileStoreResourcePath(fileResourceId), false)
		exactFileResourcePath, err := k8s.store.NewestFileOfType(fileResourcePath, Parquet)
		k8s.logger.Debugw("Retrieved latest file path", "exactFileResourcePath", exactFileResourcePath)
		if err != nil {
			k8s.logger.Errorw("Could not get newest blob", "location", fileResourcePath, "error", err)
			return "", fmt.Errorf("could not get newest blob: %s: %v", fileResourcePath, err)
		}
		if exactFileResourcePath == "" {
			k8s.logger.Errorw("Issue getting transformation table", "id", fileResourceId)
			return "", fmt.Errorf("could not get the transformation table for {%v} at {%s}", fileResourceId, fileResourcePath)
		}
		filePath := k8s.store.PathWithPrefix(exactFileResourcePath[:strings.LastIndex(exactFileResourcePath, "/")+1], false)
		return filePath, nil
	} else {
		return filePath, fmt.Errorf("could not find path for %s; fileType: %s, fileName: %s, fileVariant: %s", path, fileType, fileName, fileVariant)
	}
}

func (k8s *K8sOfflineStore) getResourceInformationFromFilePath(path string) (string, string, string) {
	var fileType string
	var fileName string
	var fileVariant string
	if path[:5] == "s3://" {
		filePaths := strings.Split(path[len("s3://"):], "/")
		if len(filePaths) <= 4 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = strings.ToLower(filePaths[2]), filePaths[3], filePaths[4]
	} else if path[:5] == HDFSPrefix {
		filePaths := strings.Split(path[len(HDFSPrefix):], "/")
		if len(filePaths) <= 4 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = strings.ToLower(filePaths[2]), filePaths[3], filePaths[4]
	} else {
		filePaths := strings.Split(path[len("featureform_"):], "__")
		if len(filePaths) <= 2 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = filePaths[0], filePaths[1], filePaths[2]
	}
	return fileType, fileName, fileVariant
}

func (k8s *K8sOfflineStore) GetTransformationTable(id ResourceID) (TransformationTable, error) {
	transformationPath := k8s.store.PathWithPrefix(fileStoreResourcePath(id), false)
	k8s.logger.Debugw("Retrieved transformation source", "ResourceId", id, "transformationPath", transformationPath)
	return &FileStorePrimaryTable{k8s.store, transformationPath, true, id}, nil
}

func (k8s *K8sOfflineStore) UpdateTransformation(config TransformationConfig) error {
	return k8s.transformation(config, true)
}
func (k8s *K8sOfflineStore) CreatePrimaryTable(id ResourceID, schema TableSchema) (PrimaryTable, error) {
	return nil, fmt.Errorf("not implemented")
}

func (k8s *K8sOfflineStore) GetPrimaryTable(id ResourceID) (PrimaryTable, error) {
	return fileStoreGetPrimary(id, k8s.store, k8s.logger)
}

func fileStoreGetPrimary(id ResourceID, store FileStore, logger *zap.SugaredLogger) (PrimaryTable, error) {
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	logger.Debugw("Getting primary table", "id", id)

	table, err := store.Read(resourceKey)
	if err != nil {
		return nil, fmt.Errorf("error fetching primary table: %v", err)
	}

	logger.Debugw("Successfully retrieved primary table", "id", id)
	return &FileStorePrimaryTable{store, string(table), false, id}, nil
}

func (k8s *K8sOfflineStore) CreateResourceTable(id ResourceID, schema TableSchema) (OfflineTable, error) {
	return nil, fmt.Errorf("not implemented")
}

func (k8s *K8sOfflineStore) GetResourceTable(id ResourceID) (OfflineTable, error) {
	return fileStoreGetResourceTable(id, k8s.store, k8s.logger)
}

func fileStoreGetResourceTable(id ResourceID, store FileStore, logger *zap.SugaredLogger) (OfflineTable, error) {
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	logger.Debugw("Getting resource table", "id", id, "resourceKey", resourceKey)
	serializedSchema, err := store.Read(resourceKey)
	if err != nil {
		return nil, fmt.Errorf("error reading schema bytes from blob storage: %v", err)
	}
	resourceSchema := ResourceSchema{}
	if err := resourceSchema.Deserialize(serializedSchema); err != nil {
		return nil, fmt.Errorf("error deserializing resource table: %v", err)
	}
	logger.Debugw("Successfully fetched resource table", "id", id)
	return &BlobOfflineTable{resourceSchema}, nil
}

func (k8s *K8sOfflineStore) CreateMaterialization(id ResourceID) (Materialization, error) {
	return k8s.materialization(id, false)
}

func (k8s *K8sOfflineStore) GetMaterialization(id MaterializationID) (Materialization, error) {
	return fileStoreGetMaterialization(id, k8s.store, k8s.logger)
}

func fileStoreGetMaterialization(id MaterializationID, store FileStore, logger *zap.SugaredLogger) (Materialization, error) {
	s := strings.Split(string(id), "/")
	if len(s) != 3 {
		logger.Errorw("Invalid materialization", "id", id)
		return nil, fmt.Errorf("invalid materialization id: %v", id)
	}
	materializationID := ResourceID{s[1], s[2], FeatureMaterialization}
	logger.Debugw("Getting materialization", "id", id)
	materializationPath := store.PathWithPrefix(fileStoreResourcePath(materializationID), false)
	materializationExactPath, err := store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		logger.Errorw("Could not fetch materialization resource key", "error", err)
		return nil, fmt.Errorf("could not fetch materialization resource key: %v", err)
	}
	logger.Debugw("Successfully retrieved materialization", "id", id)
	return &FileStoreMaterialization{materializationID, store, materializationExactPath}, nil
}

type FileStoreMaterialization struct {
	id    ResourceID
	store FileStore
	key   string
}

func (mat FileStoreMaterialization) ID() MaterializationID {
	return MaterializationID(fmt.Sprintf("%s/%s/%s", FeatureMaterialization, mat.id.Name, mat.id.Variant))
}

func (mat FileStoreMaterialization) NumRows() (int64, error) {
	materializationPath := mat.store.PathWithPrefix(fileStoreResourcePath(mat.id), false)
	latestMaterializationPath, err := mat.store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		return 0, fmt.Errorf("could not get materialization num rows; %v", err)
	}
	return mat.store.NumRows(latestMaterializationPath)
}

func (mat FileStoreMaterialization) IterateSegment(begin, end int64) (FeatureIterator, error) {
	materializationPath := mat.store.PathWithPrefix(fileStoreResourcePath(mat.id), false)
	latestMaterializationPath, err := mat.store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		return nil, fmt.Errorf("could not get materialization iterate segment: %v", err)
	}
	iter, err := mat.store.Serve(latestMaterializationPath)
	if err != nil {
		return nil, err
	}
	for i := int64(0); i < begin; i++ {
		_, _ = iter.Next()
	}
	return &FileStoreFeatureIterator{
		iter:   iter,
		curIdx: 0,
		maxIdx: end,
	}, nil
}

type FileStoreFeatureIterator struct {
	iter   Iterator
	err    error
	cur    ResourceRecord
	curIdx int64
	maxIdx int64
}

func (iter *FileStoreFeatureIterator) Next() bool {
	iter.curIdx += 1
	if iter.curIdx > iter.maxIdx {
		return false
	}
	nextVal, err := iter.iter.Next()
	if err != nil {
		iter.err = err
		return false
	}
	if nextVal == nil {
		return false
	}
	value, err := iter.parseValue(nextVal["value"])
	if err != nil {
		iter.err = err
		return false
	}
	ts, hasTimestamp := nextVal["ts"].(string)
	var timestamp time.Time
	if hasTimestamp {
		timestamp, err = iter.parseTimestamp(ts)
		if err != nil {
			iter.err = err
			return false
		}
	}
	rec := ResourceRecord{
		Value: value,
		TS:    timestamp,
	}
	if err := rec.SetEntity(nextVal["entity"]); err != nil {
		iter.err = err
		return false
	}
	iter.cur = rec
	return true
}

// Attempts to parse timestamp in one of the following formats:
// 1. "2006-01-02 15:04:05.000000 UTC"
// 2. "2006-01-02 15:04:05.000000"
// 3. "2006-01-02 15:04:05.000000 +0000 UTC"
// If any one of the three formats is valid, returns the parsed timestamp, otherwise it
// returns an error
func (iter *FileStoreFeatureIterator) parseTimestamp(ts string) (time.Time, error) {
	formats := []string{
		fmt.Sprintf("%s UTC", baseDateFormat),
		baseDateFormat,
		fmt.Sprintf("%s +0000 UTC", baseDateFormat),
	}
	for _, format := range formats {
		timestamp, err := time.Parse(format, ts)
		if err == nil {
			return timestamp, nil
		}
	}
	return time.Time{}, fmt.Errorf("could not parse timestamp: %v", ts)
}

// Attempts to parse value in one of the following formats:
// 1. a scalar value (string, int, float, bool)
// 2. []float32 (i.e. vector32)
func (iter *FileStoreFeatureIterator) parseValue(value interface{}) (interface{}, error) {
	valueMap, ok := value.(map[string]interface{})
	if !ok {
		return value, nil
	}
	list, ok := valueMap["list"]
	if !ok {
		return "", fmt.Errorf("expected to find field 'list' value (type %T)", value)
	}
	// To iterate over the list and create a we need to cast it to []interface{}
	elementsSlice, ok := list.([]interface{})
	if !ok {
		return "", fmt.Errorf("could not cast type: %T to []interface{}", list)
	}
	vector32 := make([]float32, len(elementsSlice))
	for i, e := range elementsSlice {
		// To access the 'element' field, which holds the float value,
		// we need to cast it to map[string]interface{}
		m, ok := e.(map[string]interface{})
		if !ok {
			return "", fmt.Errorf("could not cast type: %T to map[string]interface{}", e)
		}
		switch element := m["element"].(type) {
		case float32:
			vector32[i] = element
		// Given floats in Python are typically 64-bit, it's possible we'll receive
		// a vector of float64
		case float64:
			vector32[i] = float32(element)
		default:
			return "", fmt.Errorf("unexpected type in parquet vector list: %T", element)
		}
	}
	return vector32, nil
}

func (iter *FileStoreFeatureIterator) Value() ResourceRecord {
	return iter.cur
}

func (iter *FileStoreFeatureIterator) Err() error {
	return iter.err
}

func (iter *FileStoreFeatureIterator) Close() error {
	return nil
}

func (k8s *K8sOfflineStore) UpdateMaterialization(id ResourceID) (Materialization, error) {
	return k8s.materialization(id, true)
}

func (k8s *K8sOfflineStore) materialization(id ResourceID, isUpdate bool) (Materialization, error) {
	if id.Type != Feature {
		k8s.logger.Errorw("Attempted to create a materialization of a non feature resource", "type", id.Type)
		return nil, fmt.Errorf("only features can be materialized")
	}
	resourceTable, err := k8s.GetResourceTable(id)
	if err != nil {
		k8s.logger.Errorw("Attempted to fetch resource table of non registered resource", "error", err)
		return nil, fmt.Errorf("resource not registered: %v", err)
	}
	k8sResourceTable, ok := resourceTable.(*BlobOfflineTable)
	if !ok {
		k8s.logger.Errorw("Could not convert resource table to blob offline table", "id", id)
		return nil, fmt.Errorf("could not convert offline table with id %v to k8sResourceTable", id)
	}
	materializationID := ResourceID{Name: id.Name, Variant: id.Variant, Type: FeatureMaterialization}
	destinationPath := k8s.store.PathWithPrefix(fileStoreResourcePath(materializationID), false)
	materializationNewestFile, err := k8s.store.NewestFileOfType(destinationPath, Parquet)
	k8s.logger.Debugw("Running Materialization", "id", id, "destinationPath", destinationPath, "materializationNewestFile", materializationNewestFile)
	materializationExists := materializationNewestFile != ""
	if err != nil {
		k8s.logger.Errorw("Could not determine whether materialization exists", err)
		return nil, fmt.Errorf("error checking if materialization exists: %v", err)
	}
	if !isUpdate && materializationExists {
		k8s.logger.Errorw("Attempted to materialize a materialization that already exists", "id", id)
		return nil, fmt.Errorf("materialization already exists")
	} else if isUpdate && !materializationExists {
		k8s.logger.Errorw("Attempted to update a materialization that does not exist", "id", id)
		return nil, fmt.Errorf("materialization does not exist")
	}
	materializationQuery := k8s.query.materializationCreate(k8sResourceTable.schema)
	sourcePath := k8s.store.PathWithPrefix(k8sResourceTable.schema.SourceTable, false)
	newestSourcePath, err := k8s.store.NewestFileOfType(sourcePath, Parquet)
	if err != nil {
		k8s.logger.Errorw("Could not determine newest source file for materialization", "sourcePath", sourcePath, "error", err)
		return nil, fmt.Errorf("error determining newest source file: %v", err)
	}
	k8sArgs := k8s.pandasRunnerArgs(destinationPath, materializationQuery, []string{newestSourcePath}, Materialize)
	k8sArgs = addResourceID(k8sArgs, id)
	if err := k8s.executor.ExecuteScript(k8sArgs, nil); err != nil {
		k8s.logger.Errorw("Job failed to run", "error", err)
		return nil, fmt.Errorf("job for materialization %v failed to run: %v", materializationID, err)
	}

	k8s.logger.Debugw("Successfully created materialization", "id", id)
	return &FileStoreMaterialization{materializationID, k8s.store, materializationNewestFile}, nil
}

func (k8s *K8sOfflineStore) DeleteMaterialization(id MaterializationID) error {
	return fileStoreDeleteMaterialization(id, k8s.store, k8s.logger)
}

func fileStoreDeleteMaterialization(id MaterializationID, store FileStore, logger *zap.SugaredLogger) error {
	s := strings.Split(string(id), "/")
	if len(s) != 3 {
		logger.Errorw("Invalid materialization id", id)
		return fmt.Errorf("invalid materialization id")
	}
	materializationID := ResourceID{s[1], s[2], FeatureMaterialization}
	materializationPath := store.PathWithPrefix(fileStoreResourcePath(materializationID), false)
	materializationExactPath, err := store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		return fmt.Errorf("materialization does not exist: %v", err)
	}
	return store.Delete(materializationExactPath)
}

func (k8s *K8sOfflineStore) CreateTrainingSet(def TrainingSetDef) error {
	return k8s.trainingSet(def, false)
}

func (k8s *K8sOfflineStore) UpdateTrainingSet(def TrainingSetDef) error {
	return k8s.trainingSet(def, true)
}

func (k8s *K8sOfflineStore) registeredResourceSchema(id ResourceID) (ResourceSchema, error) {
	k8s.logger.Debugw("Getting resource schema", "id", id)
	table, err := k8s.GetResourceTable(id)
	if err != nil {
		k8s.logger.Errorw("Resource not registered in blob store", "id", id, "error", err)
		return ResourceSchema{}, fmt.Errorf("resource not registered: %v", err)
	}
	blobResourceTable, ok := table.(*BlobOfflineTable)
	if !ok {
		k8s.logger.Errorw("could not convert offline table to blobResourceTable", "id", id)
		return ResourceSchema{}, fmt.Errorf("could not convert offline table with id %v to blobResourceTable", id)
	}
	k8s.logger.Debugw("Successfully retrieved resource schema", "id", id, "schema", blobResourceTable.schema)
	return blobResourceTable.schema, nil
}

func (k8s *K8sOfflineStore) trainingSet(def TrainingSetDef, isUpdate bool) error {
	if err := def.check(); err != nil {
		k8s.logger.Errorw("Training set definition not valid", def, err)
		return err
	}
	sourcePaths := make([]string, 0)
	featureSchemas := make([]ResourceSchema, 0)
	destinationPath := k8s.store.PathWithPrefix(fileStoreResourcePath(def.ID), false)
	trainingSetExactPath, err := k8s.store.NewestFileOfType(destinationPath, Parquet)

	k8s.logger.Debugw("Running Training Set", "id", def.ID, "destinationPath", destinationPath, "trainingSetExactPath", trainingSetExactPath)

	if err != nil {
		return fmt.Errorf("could not get training set path: %v", err)
	}
	trainingSetExists := !(trainingSetExactPath == "")
	if !isUpdate && trainingSetExists {
		k8s.logger.Errorw("Training set already exists", "id", def.ID)
		return fmt.Errorf("k8s training set already exists: %v", def.ID)
	} else if isUpdate && !trainingSetExists {
		k8s.logger.Errorw("Training set doesn't exist for update job", def.ID)
		return fmt.Errorf("training set doesn't exist for update job: %v", def.ID)
	}
	labelSchema, err := k8s.registeredResourceSchema(def.Label)
	if err != nil {
		k8s.logger.Errorw("Could not get schema of label in store", "id", def.Label, "error", err)
		return fmt.Errorf("could not get schema of label %s: %v", def.Label, err)
	}
	labelPath := labelSchema.SourceTable
	latestLabelFile, err := k8s.store.NewestFileOfType(labelPath, Parquet)
	k8s.logger.Debugw("Latest label file", "labelPath", labelPath, "latestLabelFile", latestLabelFile)
	if err != nil {
		k8s.logger.Errorw("Could not get latest label file", "error", err)
		return fmt.Errorf("could not get latest label file: %v", err)
	}
	sourcePaths = append(sourcePaths, latestLabelFile)
	for _, feature := range def.Features {
		featureSchema, err := k8s.registeredResourceSchema(feature)
		if err != nil {
			k8s.logger.Errorw("Could not get schema of feature in store", "feature", feature, "error", err)
			return fmt.Errorf("could not get schema of feature %s: %v", feature, err)
		}
		featurePath := featureSchema.SourceTable
		latestFeatureFile, err := k8s.store.NewestFileOfType(featurePath, Parquet)
		k8s.logger.Debugw("Latest feature file", "featurePath", featurePath, "latestFeatureFile", latestFeatureFile)
		if err != nil {
			k8s.logger.Errorw("Could not get latest feature file", "error", err)
			return fmt.Errorf("could not get latest feature file: %v", err)
		}
		sourcePaths = append(sourcePaths, latestFeatureFile)
		featureSchemas = append(featureSchemas, featureSchema)
	}
	trainingSetQuery := k8s.query.trainingSetCreate(def, featureSchemas, labelSchema)
	k8s.logger.Debugw("Training set query", "SourceFiles", sourcePaths)
	k8s.logger.Debugw("Source list", "list", trainingSetQuery)
	pandasArgs := k8s.pandasRunnerArgs(k8s.store.PathWithPrefix(destinationPath, false), trainingSetQuery, sourcePaths, CreateTrainingSet)
	pandasArgs = addResourceID(pandasArgs, def.ID)
	k8s.logger.Debugw("Creating training set", "definition", def)

	if err := k8s.executor.ExecuteScript(pandasArgs, nil); err != nil {
		k8s.logger.Errorw("training set job failed to run", "definition", def.ID, "error", err)
		return fmt.Errorf("job for training set %v failed to run: %v", def.ID, err)
	}
	k8s.logger.Debugw("Successfully created training set:", "definition", def)
	return nil
}

func (k8s *K8sOfflineStore) GetTrainingSet(id ResourceID) (TrainingSetIterator, error) {
	return fileStoreGetTrainingSet(id, k8s.store, k8s.logger)
}

func fileStoreGetTrainingSet(id ResourceID, store FileStore, logger *zap.SugaredLogger) (TrainingSetIterator, error) {
	if err := id.check(TrainingSet); err != nil {
		logger.Errorw("Resource is not of type training set", "error", err)
		return nil, fmt.Errorf("resource is not training set: %w", err)
	}
	resourceKeyPrefix := store.PathWithPrefix(fileStoreResourcePath(id), false)
	trainingSetExactPath, err := store.NewestFileOfType(resourceKeyPrefix, Parquet)
	if err != nil {
		return nil, fmt.Errorf("could not get training set: %v", err)
	}
	if trainingSetExactPath == "" {
		return nil, fmt.Errorf("the training set (%v at resource prefix: %s) does not exist", id, resourceKeyPrefix)
	}

	iterator, err := store.Serve(trainingSetExactPath)
	if err != nil {
		return nil, fmt.Errorf("could not serve training set: %w", err)
	}
	return &FileStoreTrainingSet{id: id, store: store, key: trainingSetExactPath, iter: iterator}, nil
}

type FileStoreTrainingSet struct {
	id       ResourceID
	store    FileStore
	key      string
	iter     Iterator
	Error    error
	features []interface{}
	label    interface{}
}

func (ts *FileStoreTrainingSet) Next() bool {
	row, err := ts.iter.Next()
	if err != nil {
		ts.Error = err
		return false
	}
	if row == nil {
		return false
	}
	featureValues := make([]interface{}, len(ts.iter.FeatureColumns()))
	for i, key := range ts.iter.FeatureColumns() {
		featureValues[i] = row[key]
	}
	ts.features = featureValues
	ts.label = row[ts.iter.LabelColumn()]
	return true
}

func (ts *FileStoreTrainingSet) Features() []interface{} {
	return ts.features
}

func (ts *FileStoreTrainingSet) Label() interface{} {
	return ts.label
}

func (ts *FileStoreTrainingSet) Err() error {
	return ts.Error
}
